{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "\n",
    "Plan - Acquire - **Prepare** - Explore - Model - Deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we are doing and why:\n",
    "\n",
    "**What:** Clean and tidy our data so that it is ready for exploration, analysis and modeling\n",
    "\n",
    "**Why:** Set ourselves up for certainty! \n",
    "\n",
    "    1) Ensure that our observations will be sound:\n",
    "        Validity of statistical and human observations\n",
    "    2) Ensure that we will not have computational errors:\n",
    "        non numerical data cells, nulls/NaNs\n",
    "    3) Protect against overfitting:\n",
    "        Ensure that have a split data structure prior to drawing conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level Roadmap:\n",
    "\n",
    "**Input:** An aquired dataset (One Pandas Dataframe) \n",
    "\n",
    "**Output:** Cleaned data split into Train, Validate, and Test sets (Three Pandas Dataframes)\n",
    "\n",
    "**Processes:** Inspect and summarize the data ---> Clean the data ---> Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train test split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# imputer from sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# filter out warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# our own acquire script:\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather our takeaways, i.e., what we are going to do when we clean:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding: Turning Categorical Values into Boolean Values (0,1)\n",
    " - We have two options: simple encoding or one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting our Work Into a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option for Missing Values: Impute\n",
    "\n",
    "We can impute values using the mean, median, mode (most frequent), or a constant value. We will use sklearn.imputer.SimpleImputer to do this.  \n",
    "\n",
    "1. Create the imputer object, selecting the strategy used to impute (mean, median or mode (strategy = 'most_frequent'). \n",
    "2. Fit to train. This means compute the mean, median, or most_frequent (i.e. mode) for each of the columns that will be imputed. Store that value in the imputer object. \n",
    "3. Transform train: fill missing values in train dataset with that value identified\n",
    "4. Transform test: fill missing values with that value identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the `SimpleImputer` object, which we will store in the variable `imputer`. In the creation of the object, we will specify the strategy to use (`mean`, `median`, `most_frequent`). Essentially, this is creating the instructions and assigning them to a variable we will reference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `Fit` the imputer to the columns in the training df.  This means that the imputer will determine the `most_frequent` value, or other value depending on the `strategy` called, for each column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. It will store that value in the imputer object to use upon calling `transform.` We will call `transform` on each of our samples to fill any missing values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will run through all of these steps, when I provide a train and test dataframe, a strategy, and a list of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blend the clean, split and impute functions into a single prep_data() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of this exercise should be the specified functions in a python script named `prepare.py`.\n",
    "Do these in your `classification_exercises.ipynb` first, then transfer to the prepare.py file. \n",
    "\n",
    "This work should all be saved in your local `classification-exercises` repo. Then add, commit, and push your changes.\n",
    "\n",
    "Using the Iris Data:  \n",
    "\n",
    "1. Use the function defined in `acquire.py` to load the iris data.  \n",
    "\n",
    "1. Drop the `species_id` and `measurement_id` columns.  \n",
    "\n",
    "1. Rename the `species_name` column to just `species`.  \n",
    "\n",
    "1. Create dummy variables of the species name. \n",
    "\n",
    "1. Create a function named `prep_iris` that accepts the untransformed iris data, and returns the data with the transformations above applied.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
