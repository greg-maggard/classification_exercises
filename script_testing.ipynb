{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ffdf0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7032fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = igetitem(value, i)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>alone</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embark_town_Queenstown</th>\n",
       "      <th>embark_town_Southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>30.189296</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass        age  sibsp  parch      fare  alone  sex_male  \\\n",
       "301         1       3  30.189296      2      0   23.2500      0         1   \n",
       "290         1       1  26.000000      0      0   78.8500      1         0   \n",
       "779         1       1  43.000000      0      1  211.3375      0         0   \n",
       "356         1       1  22.000000      0      1   55.0000      0         0   \n",
       "147         0       3   9.000000      2      2   34.3750      0         0   \n",
       "\n",
       "     embark_town_Queenstown  embark_town_Southampton  \n",
       "301                       1                        0  \n",
       "290                       0                        1  \n",
       "779                       0                        1  \n",
       "356                       0                        1  \n",
       "147                       0                        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = acquire.get_titanic_data()\n",
    "train, test, validate = prepare.prep_titanic_data(titanic_df)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a1daea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X and y model-testing dataframes:\n",
    "X_train = train.drop(columns = ['survived'])\n",
    "y_train = train.survived\n",
    "\n",
    "X_validate = validate.drop(columns = ['survived'])\n",
    "y_validate = validate.survived\n",
    "\n",
    "X_test = test.drop(columns = ['survived'])\n",
    "y_test = test.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8085ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    "    :param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    models = [\n",
    "          ('LogReg', LogisticRegression()), \n",
    "          ('RF', RandomForestClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()), \n",
    "          ('GNB', GaussianNB()),\n",
    "          ('XGB', XGBClassifier())\n",
    "        ]\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    target_names = ['malignant', 'benign']\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        dfs.append(this_df)\n",
    "        final = pd.concat(dfs, ignore_index=True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1abd176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.80      0.89      0.84       132\n",
      "      benign       0.78      0.65      0.71        82\n",
      "\n",
      "    accuracy                           0.79       214\n",
      "   macro avg       0.79      0.77      0.77       214\n",
      "weighted avg       0.79      0.79      0.79       214\n",
      "\n",
      "RF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.85      0.89      0.87       132\n",
      "      benign       0.80      0.74      0.77        82\n",
      "\n",
      "    accuracy                           0.83       214\n",
      "   macro avg       0.83      0.82      0.82       214\n",
      "weighted avg       0.83      0.83      0.83       214\n",
      "\n",
      "KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.78      0.79      0.78       132\n",
      "      benign       0.65      0.65      0.65        82\n",
      "\n",
      "    accuracy                           0.73       214\n",
      "   macro avg       0.72      0.72      0.72       214\n",
      "weighted avg       0.73      0.73      0.73       214\n",
      "\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.70      0.92      0.79       132\n",
      "      benign       0.74      0.35      0.48        82\n",
      "\n",
      "    accuracy                           0.71       214\n",
      "   macro avg       0.72      0.64      0.64       214\n",
      "weighted avg       0.71      0.71      0.67       214\n",
      "\n",
      "GNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.78      0.88      0.83       132\n",
      "      benign       0.76      0.61      0.68        82\n",
      "\n",
      "    accuracy                           0.78       214\n",
      "   macro avg       0.77      0.74      0.75       214\n",
      "weighted avg       0.77      0.78      0.77       214\n",
      "\n",
      "XGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.84      0.89      0.86       132\n",
      "      benign       0.80      0.72      0.76        82\n",
      "\n",
      "    accuracy                           0.82       214\n",
      "   macro avg       0.82      0.80      0.81       214\n",
      "weighted avg       0.82      0.82      0.82       214\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision_weighted</th>\n",
       "      <th>test_recall_weighted</th>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028522</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.796183</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.850265</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015433</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.831533</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>0.879136</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014549</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.831459</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.822750</td>\n",
       "      <td>0.867253</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011983</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.822460</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.814574</td>\n",
       "      <td>0.860485</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011785</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.780207</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.778672</td>\n",
       "      <td>0.817735</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.074942</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.824571</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.816671</td>\n",
       "      <td>0.900857</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.077353</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.785760</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.787102</td>\n",
       "      <td>0.843520</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.077810</td>\n",
       "      <td>0.013027</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.772662</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.745804</td>\n",
       "      <td>0.846593</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.075337</td>\n",
       "      <td>0.014307</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848154</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848231</td>\n",
       "      <td>0.893066</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.080112</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.788330</td>\n",
       "      <td>0.837179</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.678010</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.671851</td>\n",
       "      <td>0.735414</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.005725</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.715619</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.717513</td>\n",
       "      <td>0.748621</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.731858</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.730855</td>\n",
       "      <td>0.728132</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.005690</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.683617</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>0.681693</td>\n",
       "      <td>0.761696</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.642136</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.643705</td>\n",
       "      <td>0.677137</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.008607</td>\n",
       "      <td>0.008110</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.692333</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.556342</td>\n",
       "      <td>0.747042</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.690705</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.690756</td>\n",
       "      <td>0.769761</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008553</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.668233</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.621457</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008457</td>\n",
       "      <td>0.008382</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.695199</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.564326</td>\n",
       "      <td>0.725982</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.008454</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.654015</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.615709</td>\n",
       "      <td>0.624145</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.828774</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.815416</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.831533</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.830678</td>\n",
       "      <td>0.865809</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.798430</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.774987</td>\n",
       "      <td>0.839560</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.001266</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.730640</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.728289</td>\n",
       "      <td>0.748956</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.748703</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.748012</td>\n",
       "      <td>0.804915</td>\n",
       "      <td>GNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.175900</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.812552</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.807078</td>\n",
       "      <td>0.884537</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.254010</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.776762</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.778046</td>\n",
       "      <td>0.855469</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.157590</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.804885</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.793570</td>\n",
       "      <td>0.844835</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.164340</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.828954</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.828533</td>\n",
       "      <td>0.890769</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.221840</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.786075</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.786223</td>\n",
       "      <td>0.856624</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_accuracy  test_precision_weighted  \\\n",
       "0   0.028522    0.006580       0.780000                 0.796183   \n",
       "1   0.015433    0.004219       0.830000                 0.831533   \n",
       "2   0.014549    0.003617       0.820000                 0.831459   \n",
       "3   0.011983    0.003132       0.818182                 0.822460   \n",
       "4   0.011785    0.003122       0.777778                 0.780207   \n",
       "5   0.074942    0.012431       0.820000                 0.824571   \n",
       "6   0.077353    0.013072       0.790000                 0.785760   \n",
       "7   0.077810    0.013027       0.740000                 0.772662   \n",
       "8   0.075337    0.014307       0.848485                 0.848154   \n",
       "9   0.080112    0.013403       0.787879                 0.788945   \n",
       "10  0.001479    0.006099       0.680000                 0.678010   \n",
       "11  0.001039    0.005725       0.720000                 0.715619   \n",
       "12  0.001254    0.005882       0.730000                 0.731858   \n",
       "13  0.001270    0.005690       0.686869                 0.683617   \n",
       "14  0.001090    0.005352       0.646465                 0.642136   \n",
       "15  0.008607    0.008110       0.630000                 0.692333   \n",
       "16  0.008799    0.008430       0.710000                 0.690705   \n",
       "17  0.008553    0.008906       0.680000                 0.668233   \n",
       "18  0.008457    0.008382       0.636364                 0.695199   \n",
       "19  0.008454    0.008317       0.656566                 0.654015   \n",
       "20  0.001581    0.003229       0.820000                 0.828774   \n",
       "21  0.001149    0.003061       0.830000                 0.831533   \n",
       "22  0.001069    0.003444       0.770000                 0.798430   \n",
       "23  0.001266    0.003217       0.727273                 0.730640   \n",
       "24  0.001084    0.002998       0.747475                 0.748703   \n",
       "25  0.175900    0.005454       0.810000                 0.812552   \n",
       "26  0.254010    0.006462       0.780000                 0.776762   \n",
       "27  0.157590    0.004996       0.790000                 0.804885   \n",
       "28  0.164340    0.005174       0.828283                 0.828954   \n",
       "29  0.221840    0.005036       0.787879                 0.786075   \n",
       "\n",
       "    test_recall_weighted  test_f1_weighted  test_roc_auc   model  \n",
       "0               0.780000          0.770642      0.850265  LogReg  \n",
       "1               0.830000          0.830678      0.879136  LogReg  \n",
       "2               0.820000          0.822750      0.867253  LogReg  \n",
       "3               0.818182          0.814574      0.860485  LogReg  \n",
       "4               0.777778          0.778672      0.817735  LogReg  \n",
       "5               0.820000          0.816671      0.900857      RF  \n",
       "6               0.790000          0.787102      0.843520      RF  \n",
       "7               0.740000          0.745804      0.846593      RF  \n",
       "8               0.848485          0.848231      0.893066      RF  \n",
       "9               0.787879          0.788330      0.837179      RF  \n",
       "10              0.680000          0.671851      0.735414     KNN  \n",
       "11              0.720000          0.717513      0.748621     KNN  \n",
       "12              0.730000          0.730855      0.728132     KNN  \n",
       "13              0.686869          0.681693      0.761696     KNN  \n",
       "14              0.646465          0.643705      0.677137     KNN  \n",
       "15              0.630000          0.556342      0.747042     SVM  \n",
       "16              0.710000          0.690756      0.769761     SVM  \n",
       "17              0.680000          0.621457      0.691429     SVM  \n",
       "18              0.636364          0.564326      0.725982     SVM  \n",
       "19              0.656566          0.615709      0.624145     SVM  \n",
       "20              0.820000          0.815416      0.852713     GNB  \n",
       "21              0.830000          0.830678      0.865809     GNB  \n",
       "22              0.770000          0.774987      0.839560     GNB  \n",
       "23              0.727273          0.728289      0.748956     GNB  \n",
       "24              0.747475          0.748012      0.804915     GNB  \n",
       "25              0.810000          0.807078      0.884537     XGB  \n",
       "26              0.780000          0.778046      0.855469     XGB  \n",
       "27              0.790000          0.793570      0.844835     XGB  \n",
       "28              0.828283          0.828533      0.890769     XGB  \n",
       "29              0.787879          0.786223      0.856624     XGB  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_exps(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4045ba62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/lzs3plw14ms00nxw2vwq68vc0000gn/T/ipykernel_71158/1875444114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'final' is not defined"
     ]
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "669d13a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/lzs3plw14ms00nxw2vwq68vc0000gn/T/ipykernel_71158/4233407743.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbootstraps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbootstrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbootstraps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final' is not defined"
     ]
    }
   ],
   "source": [
    "bootstraps = []\n",
    "for model in list(set(final.model.values)):\n",
    "    model_df = final.loc[final.model == model]\n",
    "    bootstrap = model_df.sample(n=30, replace=True)\n",
    "    bootstraps.append(bootstrap)\n",
    "        \n",
    "bootstrap_df = pd.concat(bootstraps, ignore_index=True)\n",
    "results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')\n",
    "time_metrics = ['fit_time','score_time'] # fit time metrics\n",
    "## PERFORMANCE METRICS\n",
    "results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data\n",
    "results_long_nofit = results_long_nofit.sort_values(by='values')\n",
    "## TIME METRICS\n",
    "results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data\n",
    "results_long_fit = results_long_fit.sort_values(by='values')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
