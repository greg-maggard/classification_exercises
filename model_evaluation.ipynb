{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053e3fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b2f36",
   "metadata": {},
   "source": [
    "# Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bb58e",
   "metadata": {},
   "source": [
    "## 2. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8b3bb",
   "metadata": {},
   "source": [
    "### a. In the context of this problem, what is a false positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d15186",
   "metadata": {},
   "source": [
    "A false positive would be predicting that it's a dog, but it is actually a cat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a030d929",
   "metadata": {},
   "source": [
    "### b. In the context of this problem, what is a false negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00266774",
   "metadata": {},
   "source": [
    "A false negative would be predicting that it's a cat, but it is actually a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bec37",
   "metadata": {},
   "source": [
    "### c. How would you describe this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0593ae",
   "metadata": {},
   "source": [
    "I'm not really sure what is being asked here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b74cc1",
   "metadata": {},
   "source": [
    "## 3. You are working as a data scientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "## Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found [here](https://ds.codeup.com/data/c3.csv).\n",
    "\n",
    "## Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1534aa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual     model1     model2     model3\n",
       "0    No Defect  No Defect     Defect  No Defect\n",
       "1    No Defect  No Defect     Defect     Defect\n",
       "2    No Defect  No Defect     Defect  No Defect\n",
       "3    No Defect     Defect     Defect     Defect\n",
       "4    No Defect  No Defect     Defect  No Defect\n",
       "..         ...        ...        ...        ...\n",
       "195  No Defect  No Defect     Defect     Defect\n",
       "196     Defect     Defect  No Defect  No Defect\n",
       "197  No Defect  No Defect  No Defect  No Defect\n",
       "198  No Defect  No Defect     Defect     Defect\n",
       "199  No Defect  No Defect  No Defect     Defect\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset:\n",
    "\n",
    "defects_df = pd.read_csv('cody_defects_data.csv')\n",
    "defects_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5245eb",
   "metadata": {},
   "source": [
    "### a. An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239ae6b",
   "metadata": {},
   "source": [
    "If we're wanting to ensure that we get as many defective ducks as possible, that means that we want all positive cases, and would likely be willing to tolerate some false positives. Therefore, we would want to use recall as our metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbad5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for model1 is 0.5. \n",
      "\n",
      "Recall for model2 is 0.5625. \n",
      "\n",
      "Recall for model3 is 0.8125. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Determining which model is best:\n",
    "\n",
    "models = ['model1', 'model2', 'model3']\n",
    "\n",
    "for model in models:\n",
    "    true_positive = defects_df[(defects_df[model] == \"Defect\") & (defects_df['actual'] == \"Defect\")].shape[0]\n",
    "    false_negative = defects_df[(defects_df[model] == \"No Defect\") & (defects_df['actual'] == \"Defect\")].shape[0]\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    print(f\"Recall for {model} is {recall}. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510626f",
   "metadata": {},
   "source": [
    "The above shows that the best model to use would be Model 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219de19",
   "metadata": {},
   "source": [
    "### b. Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480525e4",
   "metadata": {},
   "source": [
    "Since a false positive here would be costly, but we also don't want to have false negatives due to complaints, we would use an F1 metric. If we don't catch defects, we get complaints. If we claim something is defective when it is not, then we're paying for trips to Hawaii. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82f9be3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for model1 is 0.6153846153846154. \n",
      "\n",
      "The f1 score for model2 is 0.169811320754717. \n",
      "\n",
      "The f1 score for model3 is 0.22608695652173916. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    true_positive = defects_df[(defects_df[model] == \"Defect\") & (defects_df['actual'] == \"Defect\")].shape[0]\n",
    "    true_negative = defects_df[(defects_df[model] == \"No Defect\") & (defects_df['actual'] == \"No Defect\")].shape[0]\n",
    "    false_negative = defects_df[(defects_df[model] == \"No Defect\") & (defects_df['actual'] == \"Defect\")].shape[0]\n",
    "    false_positive = defects_df[(defects_df[model] == \"Defect\") & (defects_df['actual'] == \"No Defect\")].shape[0]\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    f1 = (2 * ((precision * recall)/(precision + recall)))\n",
    "    print(f\"The f1 score for {model} is {f1}. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a07d6",
   "metadata": {},
   "source": [
    "Based on the above, it appears that the best model is Model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246f78fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for model1 is 0.8. \n",
      "\n",
      "The precision score for model2 is 0.1. \n",
      "\n",
      "The precision score for model3 is 0.13131313131313133. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If we were going for precision:\n",
    "\n",
    "for model in models:\n",
    "    true_positive = defects_df[(defects_df[model] == \"Defect\") & (defects_df['actual'] == \"Defect\")].shape[0]\n",
    "    false_positive = defects_df[(defects_df[model] == \"Defect\") & (defects_df['actual'] == \"No Defect\")].shape[0]\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    print(f\"The precision score for {model} is {precision}. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d210a",
   "metadata": {},
   "source": [
    "If we're going for precision, minimizing false positives, then we would go with Model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236f031",
   "metadata": {},
   "source": [
    "## 4. You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "## At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). \n",
    "\n",
    "## Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "## Several models have already been developed with the data, and you can find their results [here](https://ds.codeup.com/data/gives_you_paws.csv).\n",
    "\n",
    "## Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854877a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws_df = pd.read_csv('gives_you_paws_data.csv')\n",
    "\n",
    "paws_model = ['model1', 'model2', 'model3', 'model4']\n",
    "paws_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de90446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since 'dog' is the most common, our baseline model would predict 'dog' every time and have a score of 0.6508.\n"
     ]
    }
   ],
   "source": [
    "#Creating the baseline model:\n",
    "paws_df['actual'].value_counts()\n",
    "\n",
    "baseline_model_score = (paws_df[paws_df['actual'] == 'dog'].shape[0]) / (paws_df.actual.shape[0])\n",
    "print(f\"Since 'dog' is the most common, our baseline model would predict 'dog' every time and have a score of {baseline_model_score}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611ec19",
   "metadata": {},
   "source": [
    "### a. In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cec5d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for model1 is 0.8074. \n",
      "\n",
      "The accuracy score for model2 is 0.6304. \n",
      "\n",
      "The accuracy score for model3 is 0.5096. \n",
      "\n",
      "The accuracy score for model4 is 0.7426. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in paws_model:\n",
    "    true_positive = paws_df[(paws_df[model] == \"dog\") & (paws_df['actual'] == \"dog\")].shape[0]\n",
    "    true_negative = paws_df[(paws_df[model] == \"cat\") & (paws_df['actual'] == \"cat\")].shape[0]\n",
    "    false_negative = paws_df[(paws_df[model] == \"cat\") & (paws_df['actual'] == \"dog\")].shape[0]\n",
    "    false_positive = paws_df[(paws_df[model] == \"dog\") & (paws_df['actual'] == \"cat\")].shape[0]\n",
    "    accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "    print(f\"The accuracy score for {model} is {accuracy}. \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfed827",
   "metadata": {},
   "source": [
    "Based on the above, it appears that models 1 and 4 are better than baseline in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be3e64",
   "metadata": {},
   "source": [
    "### b. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad83c2",
   "metadata": {},
   "source": [
    "Calculate recall for each model for phase 1 and select the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5642fb",
   "metadata": {},
   "source": [
    "Calculate precision for each model for phase 2 and select the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61a95e",
   "metadata": {},
   "source": [
    "### c. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend for Phase I? For Phase II?\n",
    "\n",
    "- positive = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e92261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = classification_report(df.actual, df.model1, labels = ['cat', 'dog'], output_dict = True)\n",
    "\n",
    "pd.Dataframe(x).T\n",
    "#This will convert the output into a DataFrame for legibility\n",
    "\n",
    "#I think I will need to go back and create separate dataframes, and then run a similar script for each of the models. Model 4 will be the best, if done correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bbb51",
   "metadata": {},
   "source": [
    "## 5. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "[sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) <br>\n",
    "[sklearn.metrics.precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) <br>\n",
    "[sklearn.metrics.recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)<br>\n",
    "[sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb8d6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc35cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4b391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948dc097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
